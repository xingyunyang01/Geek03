import streamlit as st
import json
from typing import Dict, Any, List
from langchain_core.messages import HumanMessage, AIMessage
from agent.graph import graph
from agent.configuration import Configuration
import time

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ·±åº¦ç ”ç©¶åŠ©æ‰‹",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–session state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "config" not in st.session_state:
    st.session_state.config = Configuration()

if "execution_steps" not in st.session_state:
    st.session_state.execution_steps = []

def reset_conversation():
    """é‡ç½®å¯¹è¯å†å²"""
    st.session_state.messages = []
    st.session_state.execution_steps = []

def update_config():
    """æ›´æ–°é…ç½®"""
    try:
        st.session_state.config = Configuration(
            query_generator_model=st.session_state.query_generator_model,
            reflection_model=st.session_state.reflection_model,
            answer_model=st.session_state.answer_model,
            number_of_initial_queries=st.session_state.number_of_initial_queries,
            max_research_loops=st.session_state.max_research_loops
        )
        st.success("é…ç½®å·²æ›´æ–°ï¼")
    except Exception as e:
        st.error(f"é…ç½®æ›´æ–°å¤±è´¥: {str(e)}")

def stream_graph_execution(user_input: str):
    """æµå¼æ‰§è¡Œgraphå¹¶æ”¶é›†æ‰§è¡Œæ­¥éª¤"""
    st.session_state.execution_steps = []
    
    # å‡†å¤‡é…ç½®
    config = {
        "configurable": {
            "query_generator_model": st.session_state.config.query_generator_model,
            "reflection_model": st.session_state.config.reflection_model,
            "answer_model": st.session_state.config.answer_model,
            "number_of_initial_queries": st.session_state.config.number_of_initial_queries,
            "max_research_loops": st.session_state.config.max_research_loops
        }
    }
    
    try:
        # æµå¼æ‰§è¡Œ
        for event in graph.stream(
            {"messages": [HumanMessage(content=user_input)]},
            config=config
        ):
            for node_name, node_output in event.items():
                if node_name != "__end__":
                    step_info = {
                        "node": node_name,
                        "output": node_output,
                        "timestamp": time.time()
                    }
                    st.session_state.execution_steps.append(step_info)
                    yield step_info
    except Exception as e:
        # è®°å½•é”™è¯¯ä¿¡æ¯
        error_step = {
            "node": "error",
            "output": {"error": str(e)},
            "timestamp": time.time()
        }
        st.session_state.execution_steps.append(error_step)
        yield error_step

def render_execution_step(step: Dict[str, Any]):
    """æ¸²æŸ“å•ä¸ªæ‰§è¡Œæ­¥éª¤"""
    # æ ¹æ®èŠ‚ç‚¹ç±»å‹é€‰æ‹©ä¸åŒçš„å›¾æ ‡
    node_icons = {
        "generate_query": "ğŸ”",
        "web_research": "ğŸŒ", 
        "reflection": "ğŸ¤”",
        "evaluate_research": "âš–ï¸",
        "finalize_answer": "ğŸ“",
        "error": "âŒ"
    }
    
    icon = node_icons.get(step['node'], "ğŸ”§")
    
    with st.expander(f"{icon} {step['node']}", expanded=True):
        st.write(f"**æ‰§è¡Œæ—¶é—´:** {time.strftime('%H:%M:%S', time.localtime(step['timestamp']))}")
        
        # å¤„ç†é”™è¯¯æƒ…å†µ
        if step['node'] == 'error':
            st.error(f"æ‰§è¡Œå‡ºé”™: {step['output'].get('error', 'æœªçŸ¥é”™è¯¯')}")
            return
        
        if isinstance(step['output'], dict):
            for key, value in step['output'].items():
                if key == "search_query" and isinstance(value, list):
                    st.write(f"**{key}:**")
                    for i, query in enumerate(value, 1):
                        st.code(f"{i}. {query}")
                elif key == "web_research_result" and isinstance(value, list):
                    st.write(f"**{key}:**")
                    for i, result in enumerate(value, 1):
                        with st.expander(f"æœç´¢ç»“æœ {i}"):
                            st.markdown(result)
                elif key == "messages" and isinstance(value, list):
                    st.write(f"**{key}:**")
                    for msg in value:
                        if hasattr(msg, 'content'):
                            st.markdown(msg.content)
                elif key == "is_sufficient":
                    st.write(f"**{key}:** {'âœ… ä¿¡æ¯å……è¶³' if value else 'âŒ éœ€è¦æ›´å¤šä¿¡æ¯'}")
                elif key == "knowledge_gap":
                    st.write(f"**{key}:**")
                    st.info(value)
                elif key == "follow_up_queries" and isinstance(value, list):
                    st.write(f"**{key}:**")
                    for i, query in enumerate(value, 1):
                        st.code(f"{i}. {query}")
                else:
                    st.write(f"**{key}:**")
                    if isinstance(value, str) and len(value) > 200:
                        with st.expander("æŸ¥çœ‹è¯¦ç»†å†…å®¹"):
                            st.markdown(value)
                    else:
                        st.write(value)
        else:
            st.write(step['output'])

# ä¾§è¾¹æ  - é…ç½®é¢æ¿
with st.sidebar:
    st.title("âš™ï¸ é…ç½®è®¾ç½®")
    
    # é…ç½®é¢„è®¾
    st.subheader("ğŸš€ å¿«é€Ÿé¢„è®¾")
    preset = st.selectbox(
        "é€‰æ‹©é…ç½®æ¨¡å¼",
        ["è‡ªå®šä¹‰", "å¿«é€Ÿæ¨¡å¼", "æ ‡å‡†æ¨¡å¼", "æ·±åº¦æ¨¡å¼"],
        help="å¿«é€Ÿæ¨¡å¼ï¼š1ä¸ªæŸ¥è¯¢ï¼Œ1è½®å¾ªç¯ï¼ˆé€‚åˆç®€å•é—®é¢˜ï¼‰\næ ‡å‡†æ¨¡å¼ï¼š3ä¸ªæŸ¥è¯¢ï¼Œ2è½®å¾ªç¯ï¼ˆå¹³è¡¡æ€§èƒ½å’Œæ•ˆæœï¼‰\næ·±åº¦æ¨¡å¼ï¼š5ä¸ªæŸ¥è¯¢ï¼Œ3è½®å¾ªç¯ï¼ˆé€‚åˆå¤æ‚ç ”ç©¶ï¼‰"
    )
    
    if preset == "å¿«é€Ÿæ¨¡å¼":
        st.session_state.query_generator_model = "deepseek-chat"
        st.session_state.reflection_model = "deepseek-chat"
        st.session_state.answer_model = "deepseek-chat"
        st.session_state.number_of_initial_queries = 1
        st.session_state.max_research_loops = 1
    elif preset == "æ ‡å‡†æ¨¡å¼":
        st.session_state.query_generator_model = "deepseek-chat"
        st.session_state.reflection_model = "deepseek-chat"
        st.session_state.answer_model = "deepseek-chat"
        st.session_state.number_of_initial_queries = 3
        st.session_state.max_research_loops = 2
    elif preset == "æ·±åº¦æ¨¡å¼":
        st.session_state.query_generator_model = "deepseek-chat"
        st.session_state.reflection_model = "deepseek-chat"
        st.session_state.answer_model = "deepseek-chat"
        st.session_state.number_of_initial_queries = 5
        st.session_state.max_research_loops = 3
    
    # æ¨¡å‹é…ç½®
    st.subheader("ğŸ¤– æ¨¡å‹é…ç½®")
    st.text_input(
        "æŸ¥è¯¢ç”Ÿæˆæ¨¡å‹",
        value=st.session_state.config.query_generator_model,
        key="query_generator_model",
        help="ç”¨äºç”Ÿæˆæœç´¢æŸ¥è¯¢çš„LLMæ¨¡å‹åç§°"
    )
    st.text_input(
        "åæ€æ¨¡å‹",
        value=st.session_state.config.reflection_model,
        key="reflection_model",
        help="ç”¨äºåæ€å’Œä¿¡æ¯è¯„ä¼°çš„LLMæ¨¡å‹åç§°"
    )
    st.text_input(
        "å›ç­”æ¨¡å‹",
        value=st.session_state.config.answer_model,
        key="answer_model",
        help="ç”¨äºç”Ÿæˆæœ€ç»ˆç­”æ¡ˆçš„LLMæ¨¡å‹åç§°"
    )
    
    # å‚æ•°é…ç½®
    st.subheader("ğŸ“Š å‚æ•°é…ç½®")
    st.number_input(
        "åˆå§‹æŸ¥è¯¢æ•°é‡",
        min_value=1,
        max_value=10,
        value=st.session_state.config.number_of_initial_queries,
        key="number_of_initial_queries",
        help="è®¾ç½®åˆå§‹æœç´¢æŸ¥è¯¢çš„æ•°é‡"
    )
    st.number_input(
        "æœ€å¤§ç ”ç©¶å¾ªç¯",
        min_value=1,
        max_value=5,
        value=st.session_state.config.max_research_loops,
        key="max_research_loops",
        help="è®¾ç½®æœ€å¤§ç ”ç©¶å¾ªç¯æ¬¡æ•°"
    )
    
    # é…ç½®æŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ’¾ æ›´æ–°é…ç½®", on_click=update_config):
            pass
    with col2:
        if st.button("ğŸ”„ é‡ç½®å¯¹è¯", on_click=reset_conversation):
            pass
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    st.subheader("ğŸ“‹ å½“å‰é…ç½®")
    with st.expander("æŸ¥çœ‹è¯¦ç»†é…ç½®"):
        st.json(st.session_state.config.model_dump())

# ä¸»ç•Œé¢
st.title("ğŸ” æ·±åº¦ç ”ç©¶åŠ©æ‰‹")
st.markdown("åŸºäºLangGraphçš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ï¼Œæ”¯æŒå¤šè½®æœç´¢å’Œæ·±åº¦åˆ†æ")

# å¯¹è¯ç•Œé¢
st.subheader("ğŸ’¬ å¯¹è¯")

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„ç ”ç©¶é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å ä½ç¬¦
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # è¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # æ‰§è¡Œæ­¥éª¤æ˜¾ç¤ºåŒºåŸŸ
        execution_container = st.container()
        
        # æµå¼æ‰§è¡Œ
        step_count = 0
        total_steps = 5  # é¢„ä¼°æ€»æ­¥éª¤æ•°
        
        for step in stream_graph_execution(prompt):
            step_count += 1
            progress = min(step_count / total_steps, 1.0)
            progress_bar.progress(progress)
            
            # æ›´æ–°çŠ¶æ€æ–‡æœ¬
            status_text.text(f"æ­£åœ¨æ‰§è¡Œ: {step['node']}...")
            
            # æ›´æ–°æ‰§è¡Œæ­¥éª¤æ˜¾ç¤º
            with execution_container:
                render_execution_step(step)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆå›ç­”
            if step['node'] == 'finalize_answer' and 'messages' in step['output']:
                for msg in step['output']['messages']:
                    if hasattr(msg, 'content'):
                        full_response = msg.content
                        message_placeholder.markdown(full_response)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if step['node'] == 'error':
                full_response = f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {step['output'].get('error', 'æœªçŸ¥é”™è¯¯')}"
                message_placeholder.error(full_response)
                break
        
        # å®Œæˆè¿›åº¦æ¡
        progress_bar.progress(1.0)
        status_text.text("æ‰§è¡Œå®Œæˆï¼")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ€ç»ˆå›ç­”ï¼Œæ˜¾ç¤ºæ‰§è¡Œå®Œæˆ
        if not full_response:
            full_response = "ç ”ç©¶å®Œæˆï¼Œè¯·æŸ¥çœ‹æ‰§è¡Œæ­¥éª¤äº†è§£è¯¦ç»†è¿‡ç¨‹ã€‚"
            message_placeholder.markdown(full_response)
    
    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})

# æ‰§è¡Œæ­¥éª¤å†å²
if st.session_state.execution_steps:
    st.subheader("ğŸ“‹ æ‰§è¡Œæ­¥éª¤å†å²")
    for step in st.session_state.execution_steps:
        render_execution_step(step)

# é¡µè„š
st.markdown("---")
st.markdown("ğŸ’¡ **æç¤º:** æ‚¨å¯ä»¥åœ¨ä¾§è¾¹æ è°ƒæ•´é…ç½®å‚æ•°æ¥ä¼˜åŒ–ç ”ç©¶æ•ˆæœ") 